{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, random_split\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        self.x = df[['전류(A)', '누적_용량(Ah)', '절대값_용량(Ah)']].values\n",
    "        #self.x = np.reshape(x, (x.shape[0], 1, x.shape[1]))\n",
    "\n",
    "        self.y = df.iloc[:, 7:8].values\n",
    "        \n",
    "        self.length = len(df)\n",
    "\n",
    "    #getitem이거 왜씀?\n",
    "    def __getitem__(self, index):\n",
    "        # x = torch.FloatTensor([self.x[index]])\n",
    "        # y = torch.FloatTensor([self.y[index]])\n",
    "        # return x, y\n",
    "        feature = torch.FloatTensor([self.x[index]])\n",
    "        label = torch.FloatTensor(self.y[index])\n",
    "\n",
    "        return feature, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 3)\n",
      "float64\n",
      "(20000, 1)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(\"180801_RFB_510cy_006_DC_20000.csv\")\n",
    "print(dataset.x.shape)\n",
    "print(dataset.x.dtype)\n",
    "print(dataset.y.shape)\n",
    "print(dataset.y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size : 16000\n",
      "Validation Data Size : 2000\n",
      "Testing Data Size : 2000\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "validation_size = int(dataset_size * 0.1)\n",
    "test_size = dataset_size - train_size - validation_size\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size])\n",
    "\n",
    "print(f\"Training Data Size : {len(train_dataset)}\")\n",
    "print(f\"Validation Data Size : {len(validation_dataset)}\")\n",
    "print(f\"Testing Data Size : {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=False, drop_last=True, num_workers=4)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=128, shuffle=False, drop_last=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_of_variates = len(train_dataset)\n",
    "prediction_length = 48\n",
    "\n",
    "\n",
    "\n",
    "from transformers import InformerConfig, InformerForPrediction\n",
    "\n",
    "config = InformerConfig(\n",
    "    # in the multivariate setting, input_size is the number of variates in the time series per time step\n",
    "    input_size=num_of_variates,\n",
    "    # prediction length:\n",
    "    prediction_length=prediction_length,\n",
    "    # context length:\n",
    "    context_length=prediction_length * 2,\n",
    "    # lags value copied from 1 week before:\n",
    "    lags_sequence=[1, 24 * 7],\n",
    "    # we'll add 5 time features (\"hour_of_day\", ..., and \"age\"):\n",
    "    #num_time_features=len(time_features) + 1,\n",
    "    \n",
    "    # informer params:\n",
    "    dropout=0.1,\n",
    "    encoder_layers=6,\n",
    "    decoder_layers=4,\n",
    "    # project input from num_of_variates*len(lags_sequence)+num_time_features to:\n",
    "    d_model=64,\n",
    ")\n",
    "\n",
    "model = InformerForPrediction(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "\n",
    "epochs = 25\n",
    "loss_history = []\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)\n",
    "\n",
    "model, optimizer, train_dataloader = accelerator.prepare(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_dataloader,\n",
    ")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(\"7\")\n",
    "    for idx, batch in tqdm(enumerate(train_dataloader)):\n",
    "        print(\"9\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            static_categorical_features=batch[\"static_categorical_features\"].to(device)\n",
    "            if config.num_static_categorical_features > 0\n",
    "            else None,\n",
    "            static_real_features=batch[\"static_real_features\"].to(device)\n",
    "            if config.num_static_real_features > 0\n",
    "            else None,\n",
    "            past_time_features=batch[\"past_time_features\"].to(device),\n",
    "            past_values=batch[\"past_values\"].to(device),\n",
    "            future_time_features=batch[\"future_time_features\"].to(device),\n",
    "            future_values=batch[\"future_values\"].to(device),\n",
    "            past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
    "            future_observed_mask=batch[\"future_observed_mask\"].to(device),\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        if idx % 100 == 0:\n",
    "            print(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
