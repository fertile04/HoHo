{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch import nn, Tensor, TupleType\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, random_split\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()       \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # div_term = torch.exp(\n",
    "        #     torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        # )\n",
    "        div_term = 1 / (10000 ** ((2 * np.arange(d_model)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term[0::2])\n",
    "        pe[:, 1::2] = torch.cos(position * div_term[1::2])\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) # [5000, 1, d_model],so need seq-len <= 5000\n",
    "        #pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(self.pe[:x.size(0), :].repeat(1,x.shape[1],1).shape ,'---',x.shape)\n",
    "        # dimension 1 maybe inequal batchsize\n",
    "        return x + self.pe[:x.size(0), :].repeat(1,x.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "        input_size: int,\n",
    "        dec_seq_len: int,\n",
    "        batch_first: bool,\n",
    "        out_seq_len: int=58,\n",
    "        dim_val: int=512,  \n",
    "        n_encoder_layers: int=4,\n",
    "        n_decoder_layers: int=4,\n",
    "        n_heads: int=8,\n",
    "        dropout_encoder: float=0.2, \n",
    "        dropout_decoder: float=0.2,\n",
    "        dropout_pos_enc: float=0.1,\n",
    "        dim_feedforward_encoder: int=2048,\n",
    "        dim_feedforward_decoder: int=2048,\n",
    "        num_predicted_features: int=1\n",
    "        ):\n",
    "\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "\n",
    "        self.encoder_input_layer = nn.Linear(\n",
    "        in_features=input_size, \n",
    "        out_features=dim_val \n",
    "        )\n",
    "\n",
    "        self.decoder_input_layer = nn.Linear(\n",
    "        in_features=num_predicted_features,\n",
    "        out_features=dim_val\n",
    "        )  \n",
    "        \n",
    "        self.linear_mapping = nn.Linear(\n",
    "        in_features=dim_val, \n",
    "        out_features=num_predicted_features\n",
    "        )\n",
    "\n",
    "        # Create positional encoder\n",
    "        self.positional_encoding_layer = PositionalEncoding(\n",
    "            d_model=dim_val,\n",
    "            #dropout=dropout_pos_enc\n",
    "            )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_val, \n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_encoder,\n",
    "            dropout=dropout_encoder,\n",
    "            batch_first=batch_first\n",
    "            )\n",
    "        \n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=n_encoder_layers, \n",
    "            norm=None\n",
    "            )\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=dim_val,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_decoder,\n",
    "            dropout=dropout_decoder,\n",
    "            batch_first=batch_first\n",
    "            )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=decoder_layer,\n",
    "            num_layers=n_decoder_layers, \n",
    "            norm=None\n",
    "            )\n",
    "        \n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor=None, \n",
    "                tgt_mask: Tensor=None) -> Tensor:\n",
    "        \n",
    "        src = self.encoder_input_layer(src)\n",
    "        src = self.positional_encoding_layer(src)   \n",
    "        src = self.encoder( # src shape: [batch_size, enc_seq_len, dim_val]\n",
    "            src=src\n",
    "            )\n",
    "        \n",
    "        decoder_output = self.decoder_input_layer(tgt)\n",
    "        decoder_output = self.decoder(\n",
    "            tgt=decoder_output,\n",
    "            memory=src,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=src_mask\n",
    "            )\n",
    "        decoder_output = self.linear_mapping(decoder_output)\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Tuple\n",
    "# def get_src_tgt(\n",
    "#         self,\n",
    "#         sequence: torch.Tensor, \n",
    "#         enc_seq_len: int, \n",
    "#         target_seq_len: int\n",
    "#         ) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
    "\n",
    "#         \"\"\"\n",
    "#         Generate the src (encoder input), tgt (decoder input) and tgt_y (the target)\n",
    "#         sequences from a sequence. \n",
    "#         Args:\n",
    "#             sequence: tensor, a 1D tensor of length n where \n",
    "#                     n = encoder input length + target sequence length  \n",
    "#             enc_seq_len: int, the desired length of the input to the transformer encoder\n",
    "#             target_seq_len: int, the desired length of the target sequence (the \n",
    "#                             one against which the model output is compared)\n",
    "#         Return: \n",
    "#             src: tensor, 1D, used as input to the transformer model\n",
    "#             tgt: tensor, 1D, used as input to the transformer model\n",
    "#             tgt_y: tensor, 1D, the target sequence against which the model output\n",
    "#                 is compared when computing loss. \n",
    "        \n",
    "#         \"\"\"\n",
    "#         assert len(sequence) == enc_seq_len + target_seq_len, \"Sequence length does not equal (input length + target length)\"\n",
    "\n",
    "#         src = sequence[:enc_seq_len] \n",
    "        \n",
    "#         tgt = sequence[enc_seq_len-1:len(sequence)-1]\n",
    "#         tgt = tgt[:, 0]\n",
    "\n",
    "#         if len(tgt.shape) == 1:\n",
    "#             tgt = tgt.unsqueeze(-1)\n",
    "        \n",
    "#         assert len(tgt) == target_seq_len, \"Length of tgt does not match target sequence length\"\n",
    "\n",
    "#         tgt_y = sequence[-target_seq_len:]\n",
    "#         tgt_y = tgt_y[:, 0]\n",
    "#         assert len(tgt_y) == target_seq_len, \"Length of tgt_y does not match target sequence length\"\n",
    "\n",
    "\n",
    "#         return src, tgt, tgt_y.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(dim1: int, dim2: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Generates an upper-triangular matrix of -inf, with zeros on diag.\n",
    "    Source:\n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    Args:\n",
    "        dim1: int, for both src and tgt masking, this must be target sequence\n",
    "              length\n",
    "        dim2: int, for src masking this must be encoder sequence length (i.e. \n",
    "              the length of the input sequence to the model), \n",
    "              and for tgt masking, this must be target sequence length \n",
    "    Return:\n",
    "        A Tensor of shape [dim1, dim2]\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(dim1, dim2) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrideDataset(Dataset):\n",
    "    def __init__(self, file_path, enc_seq_len, target_seq_len, x_size, stride=5):\n",
    "        df = pd.read_csv(file_path)\n",
    "        self.x = df.iloc[x_size:, 6:10].values\n",
    "        self.y = df.iloc[x_size:, 13:14].values\n",
    "        \n",
    "        self.length = len(df) - x_size\n",
    "        \n",
    "        print(self.length)\n",
    "\n",
    "        num_samples = (self.length - enc_seq_len - target_seq_len) // stride + 1 #stride씩 움직일 때 생기는 총 sample 개수\n",
    "        \n",
    "        src = np.zeros([enc_seq_len, num_samples])\n",
    "        tgt = np.zeros([target_seq_len, num_samples])\n",
    "\n",
    "        self.x = self.x.squeeze()\n",
    "\n",
    "        for i in np.arange(num_samples):\n",
    "            start_x = stride*i\n",
    "            end_x = start_x + enc_seq_len\n",
    "            src[:,i] = self.data[start_x:end_x]\n",
    "\n",
    "            start_y = stride*i + enc_seq_len\n",
    "            end_y = start_y + target_seq_len\n",
    "            tgt[:,i] = self.data[start_y:end_y]\n",
    "\n",
    "        src = src.reshape(src.shape[0], src.shape[1], 1).transpose((1,0,2))\n",
    "        tgt = tgt.reshape(tgt.shape[0], tgt.shape[1], 1).transpose((1,0,2))\n",
    "        self.src = src\n",
    "        self.tgt = tgt\n",
    "        \n",
    "        self.len = len(src)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.src[i], self.tgt[i,:-1], self.tgt[i,1:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_path, x_size):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        self.x = df.iloc[x_size:, 6:10].values\n",
    "        #self.x = np.reshape(x, (x.shape[0], 1, x.shape[1]))\n",
    "\n",
    "        self.y = df.iloc[x_size:, 13:14].values\n",
    "        \n",
    "        self.length = len(df) - x_size\n",
    "\n",
    "    #getitem이거 왜씀?\n",
    "    def __getitem__(self, index):\n",
    "        # x = torch.FloatTensor([self.x[index]])\n",
    "        # y = torch.FloatTensor([self.y[index]])\n",
    "        # return x, y\n",
    "        feature = torch.FloatTensor([self.x[index]])\n",
    "        label = torch.FloatTensor(self.y[index])\n",
    "\n",
    "        return feature, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10639\n"
     ]
    }
   ],
   "source": [
    "train_dataset = StrideDataset(\"DST_80.csv\", 300, 75, 1919, stride=50)\n",
    "dataset_d = CustomDataset(\"DST_80.csv\",1919)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input length\n",
    "enc_seq_len = 300\n",
    "# Output length\n",
    "output_sequence_length = 75\n",
    "\n",
    "tgt_mask = generate_square_subsequent_mask(\n",
    "    dim1=output_sequence_length-1,\n",
    "    dim2=output_sequence_length-1\n",
    "   ).to(device)\n",
    "\n",
    "src_mask = generate_square_subsequent_mask(\n",
    "    dim1=output_sequence_length-1,\n",
    "    dim2=enc_seq_len\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74, 74])\n",
      "torch.Size([74, 300])\n"
     ]
    }
   ],
   "source": [
    "print(tgt_mask.shape)\n",
    "print(src_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_val = 512 # This can be any value divisible by n_heads. 512 is used in the original transformer paper.\n",
    "n_heads = 8 # The number of attention heads (aka parallel attention layers). dim_val must be divisible by this number\n",
    "n_decoder_layers = 4 # Number of times the decoder layer is stacked in the decoder\n",
    "n_encoder_layers = 4 # Number of times the encoder layer is stacked in the encoder\n",
    "input_size = 1 # The number of input variables. 1 if univariate forecasting.\n",
    "dec_seq_len = 75 # length of input given to decoder. Can have any integer value.\n",
    "enc_seq_len = 300 # length of input given to encoder. Can have any integer value.\n",
    "output_sequence_length = 75 # Length of the target sequence, i.e. how many time steps should your forecast cover\n",
    "#max_seq_len = enc_seq_len # What's the longest sequence the model will encounter? Used to make the positional encoder\n",
    "\n",
    "model = TimeSeriesTransformer(\n",
    "    dim_val=dim_val,\n",
    "    input_size=input_size, \n",
    "    dec_seq_len=dec_seq_len,\n",
    "    batch_first = True,\n",
    "    out_seq_len=output_sequence_length, \n",
    "    n_decoder_layers=n_decoder_layers,\n",
    "    n_encoder_layers=n_encoder_layers,\n",
    "    n_heads=n_heads).to(device)\n",
    "\n",
    "loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "553.62360: 100%|██████████| 100/100 [01:12<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "epoch = 100\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "progress = tqdm(range(epoch))\n",
    "\n",
    "for i in progress:\n",
    "    batchloss = 0.0\n",
    "\n",
    "    for (src, tgt, tgt_y) in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        result = model(src.float().to(device), tgt.float().to(device), src_mask, tgt_mask)\n",
    "        loss = loss_func(result, tgt_y.float().to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batchloss += loss\n",
    "        \n",
    "    progress.set_description(\"{:0.5f}\".format(batchloss.cpu().item() / len(train_dataloader)))#??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# epoch_loss = 0\n",
    "# n = 0\n",
    "\n",
    "# result_arr = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for (src, tgt, tgt_y) in train_dataloader:\n",
    "#         result = model(src.float().to(device), tgt.float().to(device), src_mask, tgt_mask)\n",
    "#         loss = loss_func(result, tgt_y.float().to(device))\n",
    "\n",
    "#         epoch_loss += loss.item()\n",
    "#         n = n+1\n",
    "#         result_arr.append(result.float().cpu())\n",
    "\n",
    "# print(n)\n",
    "# result_arr = np.array(result_arr)\n",
    "# result_arr[0].dtype\n",
    "# # result_arr = torch.Tensor(result_arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result_arr.dtype)\n",
    "# print(result_arr[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,6))\n",
    "# plt.plot(dataset_d.y, label='Actual Data')\n",
    "# plt.plot(result_arr, label='Predicted Data')\n",
    "# plt.title('SoC prediction')\n",
    "# #plt.xlim(0,2000)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = torch.tensor(train_dataset.src).to(device)\n",
    "# output  = torch.tensor(train_dataset.tgt).to(device)\n",
    "\n",
    "# model.to(device)\n",
    "# train_predict = model(input, output, src_mask, tgt_mask)\n",
    "# predicted = train_predict.data.numpy()\n",
    "\n",
    "# Y = torch.Tensor(dataset_d.y)\n",
    "\n",
    "\n",
    "# # predicted = predicted.squeeze()\n",
    "# # predicted.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
